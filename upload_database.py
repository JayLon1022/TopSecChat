# manage_index.py

import os
import yaml
import logging
import sys
import fitz  # PyMuPDF

# é…ç½®æ—¥å¿—
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
    Document,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# --- 1. å…¨å±€é…ç½® ---

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open('conf.yaml', 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def setup_services():
    """é…ç½®LlamaIndexçš„å…¨å±€æœåŠ¡ (Embeddingæ¨¡å‹)"""
    print("ğŸš€ Configuring services...")
    Settings.embed_model = HuggingFaceEmbedding(
        model_name="BAAI/bge-small-zh-v1.5",
        cache_folder="./models_cache"
    )
    print("âœ… Services configured.")

# MODIFIED: åœ¨å…ƒæ•°æ®ä¸­åŒæ—¶æ·»åŠ  file_name å’Œ file_path
def get_file_metadata(file_path: str) -> dict:
    """ä¸ºæ–‡æ¡£åˆ›å»ºå…ƒæ•°æ®ï¼ŒåŒæ—¶åŒ…å«æ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶åå’Œä¿®æ”¹æ—¶é—´"""
    return {
        "file_path": file_path,                       # ä¿ç•™å®Œæ•´è·¯å¾„ï¼Œç”¨äºåŒæ­¥é€»è¾‘
        "file_name": os.path.basename(file_path),     # <<<---ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ–°å¢ï¼šæ–‡ä»¶åï¼Œç”¨äºå‰ç«¯æ˜¾ç¤º
        "last_modified": os.path.getmtime(file_path)
    }

def is_pdf_parseable(pdf_path: str, min_text_ratio: float = 0.1) -> bool:
    """æ£€æµ‹PDFæ˜¯å¦é€‚åˆè§£æï¼ˆéå›¾ç‰‡å‹PDFï¼‰"""
    try:
        doc = fitz.open(pdf_path)
        if len(doc) == 0:
            doc.close()
            return False
        
        # é‡‡æ ·å‰3é¡µæˆ–å…¨éƒ¨é¡µé¢ï¼ˆå¦‚æœå°‘äº3é¡µï¼‰
        sample_pages = min(3, len(doc))
        total_chars = 0
        total_images = 0
        
        for page_num in range(sample_pages):
            try:
                page = doc[page_num]
                text = page.get_text()
                total_chars += len(text.strip())
                # å°è¯•è·å–å›¾ç‰‡ï¼Œå¦‚æœå¤±è´¥å°±è·³è¿‡å›¾ç‰‡è®¡æ•°
                try:
                    total_images += len(page.get_images())
                except:
                    pass
            except Exception as page_error:
                print(f"âš ï¸ Error processing page {page_num} in {pdf_path}: {page_error}")
                continue
        
        doc.close()
        
        # å¦‚æœæ–‡æœ¬å¤ªå°‘ä¸”å›¾ç‰‡å¾ˆå¤šï¼Œè®¤ä¸ºæ˜¯å›¾ç‰‡å‹PDF
        if total_chars < 100 and total_images > sample_pages * 2:
            return False
            
        # å¦‚æœå¹³å‡æ¯é¡µæ–‡æœ¬å­—ç¬¦æ•°å¤ªå°‘ï¼Œå¯èƒ½æ˜¯æ‰«æä»¶
        if sample_pages > 0:
            avg_chars_per_page = total_chars / sample_pages
            if avg_chars_per_page < 50:
                return False
        
        # å¦‚æœæ€»å­—ç¬¦æ•°å¤ªå°‘ï¼Œä¸å€¼å¾—å¤„ç†
        if total_chars < 20:
            return False
            
        return True
        
    except Exception as e:
        print(f"âš ï¸ Error checking PDF {pdf_path}: {e}")
        return False

def extract_pdf_text(pdf_path: str) -> str:
    """ä½¿ç”¨PyMuPDFæå–PDFæ–‡æœ¬ï¼Œå¸¦æœ‰é”™è¯¯æ¢å¤æœºåˆ¶"""
    text = ""
    doc = None
    
    try:
        # å°è¯•æ­£å¸¸æ‰“å¼€PDF
        doc = fitz.open(pdf_path)
        
        for page_num in range(len(doc)):
            try:
                page = doc[page_num]
                page_text = page.get_text()
                text += page_text + "\n"
            except Exception as page_error:
                print(f"âš ï¸ Error extracting text from page {page_num} in {pdf_path}: {page_error}")
                # å°è¯•ä½¿ç”¨å…¶ä»–æ–‡æœ¬æå–æ–¹æ³•
                try:
                    page_text = page.get_text("text")  # æ˜ç¡®æŒ‡å®šæ–‡æœ¬æ¨¡å¼
                    text += page_text + "\n"
                except:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•å­—å…¸æ¨¡å¼
                    try:
                        text_dict = page.get_text("dict")
                        page_text = ""
                        for block in text_dict.get("blocks", []):
                            if "lines" in block:
                                for line in block["lines"]:
                                    for span in line.get("spans", []):
                                        page_text += span.get("text", "")
                        text += page_text + "\n"
                    except:
                        print(f"âš ï¸ Completely failed to extract text from page {page_num}")
                        continue
        
        doc.close()
        
    except Exception as e:
        print(f"âš ï¸ Error opening PDF {pdf_path}: {e}")
        if doc:
            try:
                doc.close()
            except:
                pass
        return ""
    
    return text.strip()

def load_documents_with_local_parsing(knowledge_base_path: str):
    """ä½¿ç”¨æœ¬åœ°è§£æåŠ è½½æ–‡æ¡£"""
    documents = []
    skipped_pdfs = []
    processed_count = 0
    
    print(f"ğŸ“‚ Scanning directory: {knowledge_base_path}")
    
    for dirpath, _, filenames in os.walk(knowledge_base_path):
        for filename in filenames:
            file_path = os.path.join(dirpath, filename)
            
            if filename.lower().endswith('.pdf'):
                processed_count += 1
                print(f"ğŸ” Processing PDF ({processed_count}): {filename}")
                
                # æ£€æŸ¥PDFæ˜¯å¦é€‚åˆè§£æ
                if not is_pdf_parseable(file_path):
                    skipped_pdfs.append(file_path)
                    print(f"â­ï¸ Skipping image-heavy/low-text PDF: {filename}")
                    continue
                
                # æå–PDFæ–‡æœ¬
                text = extract_pdf_text(file_path)
                if text.strip():
                    metadata = get_file_metadata(file_path)
                    doc = Document(text=text, metadata=metadata)
                    documents.append(doc)
                    print(f"âœ… Successfully processed: {filename} ({len(text)} chars)")
                else:
                    skipped_pdfs.append(file_path)
                    print(f"â­ï¸ Skipping PDF with no extractable text: {filename}")
                    
            elif filename.lower().endswith(('.txt', '.md')):
                # å¤„ç†æ–‡æœ¬æ–‡ä»¶
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    if text.strip():
                        metadata = get_file_metadata(file_path)
                        doc = Document(text=text, metadata=metadata)
                        documents.append(doc)
                        print(f"âœ… Successfully processed: {filename}")
                except Exception as e:
                    print(f"âš ï¸ Error reading {file_path}: {e}")
    
    if skipped_pdfs:
        print(f"ğŸ“‹ Skipped {len(skipped_pdfs)} PDFs (image-heavy or unreadable)")
        print("Skipped files:")
        for skipped in skipped_pdfs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {os.path.basename(skipped)}")
        if len(skipped_pdfs) > 10:
            print(f"  ... and {len(skipped_pdfs) - 10} more")
    
    print(f"ğŸ“Š Successfully loaded {len(documents)} documents")
    return documents

# --- 2. æ ¸å¿ƒç´¢å¼•ç®¡ç†é€»è¾‘ ---

def create_new_index(knowledge_base_path: str, storage_path: str):
    """é¦–æ¬¡åˆ›å»ºå…¨æ–°çš„ç´¢å¼•ï¼Œä½¿ç”¨æœ¬åœ°PDFè§£æ"""
    print(f"ğŸ” No existing index found. Starting fresh indexing from '{knowledge_base_path}'...")
    
    if not os.path.exists(knowledge_base_path) or not os.listdir(knowledge_base_path):
        os.makedirs(knowledge_base_path, exist_ok=True)
        print(f"âŒ Knowledge base directory is empty. Please add documents to '{knowledge_base_path}'.")
        return

    docs = load_documents_with_local_parsing(knowledge_base_path)

    if not docs:
        print("ğŸ¤· No processable documents found in the knowledge base.")
        return

    index = VectorStoreIndex.from_documents(docs, show_progress=True)
    index.storage_context.persist(persist_dir=storage_path)
    print(f"ğŸ‰ New index created successfully with {len(docs)} documents.")

def synchronize_index(knowledge_base_path: str, storage_path: str):
    """åŒæ­¥ç°æœ‰ç´¢å¼•ï¼Œæ‰§è¡Œå¢åˆ æ”¹æ“ä½œï¼Œä½¿ç”¨æœ¬åœ°PDFè§£æ"""
    print(f"ğŸ”„ Found existing index. Synchronizing with '{knowledge_base_path}'...")
    
    storage_context = StorageContext.from_defaults(persist_dir=storage_path)
    index = load_index_from_storage(storage_context)
    
    # è·å–ç´¢å¼•ä¸­çš„æ–‡ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨è§„èŒƒåŒ–è·¯å¾„
    indexed_files = {}
    print(f"ğŸ“‹ Current indexed documents:")
    for node_id, node in index.docstore.docs.items():
        if node.metadata and "file_path" in node.metadata:
            file_path = os.path.normpath(node.metadata["file_path"])
            indexed_files[file_path] = node.metadata.get("last_modified", 0)
            print(f"  - {os.path.basename(file_path)} (path: {file_path})")
    
    # è·å–ç£ç›˜æ–‡ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨è§„èŒƒåŒ–è·¯å¾„
    disk_files = {}
    print(f"ğŸ“‚ Current disk files:")
    for dirpath, _, filenames in os.walk(knowledge_base_path):
        for filename in filenames:
            if filename.lower().endswith(('.pdf', '.txt', '.md')):
                file_path = os.path.normpath(os.path.join(dirpath, filename))
                disk_files[file_path] = get_file_metadata(file_path)["last_modified"]
                print(f"  - {filename} (path: {file_path})")

    added_files = disk_files.keys() - indexed_files.keys()
    deleted_files = indexed_files.keys() - disk_files.keys()
    updated_files = {path for path, mtime in disk_files.items() if path in indexed_files and mtime > indexed_files[path]}

    print(f"ğŸ“Š Analysis:")
    print(f"  - Added files: {len(added_files)}")
    print(f"  - Deleted files: {len(deleted_files)}")
    print(f"  - Updated files: {len(updated_files)}")

    if not added_files and not deleted_files and not updated_files:
        print("âœ… Index is already up to date.")
        return

    changes_made = False

    # å¤„ç†åˆ é™¤çš„æ–‡ä»¶ - ä½¿ç”¨ref_doc_idè¿›è¡Œåˆ é™¤
    for file_path in deleted_files:
        print(f"ğŸ—‘ï¸ Deleting: {os.path.basename(file_path)}")
        try:
            # æ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£èŠ‚ç‚¹
            nodes_to_delete = []
            for node_id, node in index.docstore.docs.items():
                if node.metadata and node.metadata.get("file_path") == file_path:
                    nodes_to_delete.append(node_id)
            
            # åˆ é™¤æ‰¾åˆ°çš„èŠ‚ç‚¹
            for node_id in nodes_to_delete:
                index.delete_ref_doc(node_id, delete_from_docstore=True)
                print(f"  Deleted node: {node_id}")
            
            if nodes_to_delete:
                changes_made = True
            else:
                print(f"  âš ï¸ No nodes found for {file_path}")
        except Exception as e:
            print(f"  âš ï¸ Error deleting {file_path}: {e}")

    # å¤„ç†æ›´æ–°çš„æ–‡ä»¶
    for file_path in updated_files:
        print(f"ğŸ”„ Updating: {os.path.basename(file_path)}")
        try:
            # å…ˆåˆ é™¤æ—§ç‰ˆæœ¬
            nodes_to_delete = []
            for node_id, node in index.docstore.docs.items():
                if node.metadata and node.metadata.get("file_path") == file_path:
                    nodes_to_delete.append(node_id)
            
            for node_id in nodes_to_delete:
                index.delete_ref_doc(node_id, delete_from_docstore=True)
            
            # é‡æ–°è§£æå’Œæ·»åŠ æ–‡ä»¶
            if file_path.lower().endswith('.pdf'):
                if is_pdf_parseable(file_path):
                    text = extract_pdf_text(file_path)
                    if text.strip():
                        doc = Document(text=text, metadata=get_file_metadata(file_path))
                        index.insert(doc)
                        changes_made = True
                        print(f"âœ… Updated PDF: {os.path.basename(file_path)}")
                    else:
                        print(f"â­ï¸ Skipped PDF (no text): {os.path.basename(file_path)}")
                else:
                    print(f"â­ï¸ Skipped PDF (not parseable): {os.path.basename(file_path)}")
            else:  # txt, md files
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                if text.strip():
                    doc = Document(text=text, metadata=get_file_metadata(file_path))
                    index.insert(doc)
                    changes_made = True
                    print(f"âœ… Updated: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"âš ï¸ Error updating {file_path}: {e}")

    # å¤„ç†æ–°å¢çš„æ–‡ä»¶
    if added_files:
        print(f"â• Adding {len(added_files)} new files...")
        new_docs = []
        for file_path in added_files:
            if file_path.lower().endswith('.pdf'):
                print(f"ğŸ” Processing new PDF: {os.path.basename(file_path)}")
                if is_pdf_parseable(file_path):
                    text = extract_pdf_text(file_path)
                    if text.strip():
                        doc = Document(text=text, metadata=get_file_metadata(file_path))
                        new_docs.append(doc)
                        print(f"âœ… Added PDF: {os.path.basename(file_path)}")
                    else:
                        print(f"â­ï¸ Skipped PDF (no text): {os.path.basename(file_path)}")
                else:
                    print(f"â­ï¸ Skipped PDF (not parseable): {os.path.basename(file_path)}")
            else:  # txt, md files
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    if text.strip():
                        doc = Document(text=text, metadata=get_file_metadata(file_path))
                        new_docs.append(doc)
                        print(f"âœ… Added: {os.path.basename(file_path)}")
                except Exception as e:
                    print(f"âš ï¸ Error reading {file_path}: {e}")
        
        if new_docs:
            for doc in new_docs:
                index.insert(doc)
            changes_made = True
    
    if changes_made:
        print("ğŸ’¾ Persisting changes to the index...")
        index.storage_context.persist(persist_dir=storage_path)
        print("âœ¨ Index synchronization complete.")

# --- 5. ä¸»ç¨‹åºå…¥å£ ---

def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ‰€æœ‰æ“ä½œ"""
    config = load_config()
    rag_config = config['rag']

    # è¯»å–ä¸¤ä¸ªçŸ¥è¯†åº“è·¯å¾„å’Œå­˜å‚¨è·¯å¾„
    # æ³¨æ„ï¼šè¿™é‡Œçš„ 'security_knowledge_base_path' ç­‰é”®åéœ€è¦ä¸ä½ çš„ conf.yaml æ–‡ä»¶å®Œå…¨å¯¹åº”
    security_knowledge_path = rag_config.get('security_knowledge_docs_path', './security_knowledge')
    security_knowledge_storage = rag_config.get('security_knowledge_storage_path', './storage/security_knowledge')
    topsec_document_knowledge_path = rag_config.get('topsec_document_docs_path', './topsec_document_knowledge')
    topsec_document_knowledge_storage = rag_config.get('topsec_document_knowledge_storage_path', './storage/topsec_document_knowledge')

    setup_services()
    
    required_files = ["docstore.json", "vector_store.json", "index_store.json"]

    # 1. å¤„ç† security_knowledge
    print("\n=== [1/2] å¤„ç†ç½‘ç»œå®‰å…¨çŸ¥è¯†åº“ ===")
    is_complete_sec = all(os.path.exists(os.path.join(security_knowledge_storage, f)) for f in required_files)
    if not is_complete_sec:
        create_new_index(security_knowledge_path, security_knowledge_storage)
    else:
        synchronize_index(security_knowledge_path, security_knowledge_storage)

    # 2. å¤„ç† topsec_document_knowledge
    print("\n=== [2/2] å¤„ç†å¤©èä¿¡å…¬å¸æ–‡æ¡£çŸ¥è¯†åº“ ===")
    is_complete_topsec = all(os.path.exists(os.path.join(topsec_document_knowledge_storage, f)) for f in required_files)
    if not is_complete_topsec:
        create_new_index(topsec_document_knowledge_path, topsec_document_knowledge_storage)
    else:
        synchronize_index(topsec_document_knowledge_path, topsec_document_knowledge_storage)

# --- 4. è°ƒè¯•åŠŸèƒ½ ---

def debug_index_content(storage_path: str, max_docs: int = 10):
    """è°ƒè¯•åŠŸèƒ½ï¼šæŸ¥çœ‹ç´¢å¼•ä¸­çš„æ–‡æ¡£å†…å®¹"""
    print(f"ğŸ” Debugging index content in: {storage_path}")
    
    if not os.path.exists(storage_path):
        print(f"âŒ Storage path does not exist: {storage_path}")
        return
    
    try:
        storage_context = StorageContext.from_defaults(persist_dir=storage_path)
        index = load_index_from_storage(storage_context)
        
        print(f"ğŸ“Š Total documents in index: {len(index.docstore.docs)}")
        
        doc_count = 0
        for node_id, node in index.docstore.docs.items():
            if doc_count >= max_docs:
                break
                
            print(f"\n--- Document {doc_count + 1} ---")
            print(f"Node ID: {node_id}")
            
            if node.metadata:
                print("Metadata:")
                for key, value in node.metadata.items():
                    print(f"  {key}: {value}")
            else:
                print("Metadata: None")
            
            # æ˜¾ç¤ºæ–‡æ¡£å†…å®¹çš„å‰100ä¸ªå­—ç¬¦
            text_preview = node.text[:100] + "..." if len(node.text) > 100 else node.text
            print(f"Text preview: {text_preview}")
            
            doc_count += 1
        
        if len(index.docstore.docs) > max_docs:
            print(f"\n... and {len(index.docstore.docs) - max_docs} more documents")
            
    except Exception as e:
        print(f"âŒ Error debugging index: {e}")

def clear_index(storage_path: str):
    """æ¸…ç©ºæŒ‡å®šçš„ç´¢å¼•ï¼ˆç”¨äºé‡å»ºï¼‰"""
    print(f"ğŸ—‘ï¸ Clearing index in: {storage_path}")
    
    if os.path.exists(storage_path):
        import shutil
        shutil.rmtree(storage_path)
        print(f"âœ… Index cleared: {storage_path}")
    else:
        print(f"âš ï¸ No index found at: {storage_path}")

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è°ƒè¯•å‚æ•°
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "debug":
            # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºç´¢å¼•å†…å®¹
            setup_services()
            config = load_config()
            rag_config = config['rag']
            
            security_storage = rag_config.get('security_knowledge_storage_path', './storage/security_knowledge')
            topsec_storage = rag_config.get('topsec_document_knowledge_storage_path', './storage/topsec_document_knowledge')
            
            print("=== ç½‘ç»œå®‰å…¨çŸ¥è¯†åº“ç´¢å¼•å†…å®¹ ===")
            debug_index_content(security_storage)
            
            print("\n=== å¤©èä¿¡æ–‡æ¡£çŸ¥è¯†åº“ç´¢å¼•å†…å®¹ ===")
            debug_index_content(topsec_storage)
            
        elif command == "clear":
            # æ¸…ç©ºç´¢å¼•æ¨¡å¼
            config = load_config()
            rag_config = config['rag']
            
            security_storage = rag_config.get('security_knowledge_storage_path', './storage/security_knowledge')
            topsec_storage = rag_config.get('topsec_document_knowledge_storage_path', './storage/topsec_document_knowledge')
            
            if len(sys.argv) > 2:
                target = sys.argv[2]
                if target == "security":
                    clear_index(security_storage)
                elif target == "topsec":
                    clear_index(topsec_storage)
                elif target == "all":
                    clear_index(security_storage)
                    clear_index(topsec_storage)
                else:
                    print("ç”¨æ³•: python upload_database.py clear [security|topsec|all]")
            else:
                print("ç”¨æ³•: python upload_database.py clear [security|topsec|all]")
        else:
            print("å¯ç”¨å‘½ä»¤:")
            print("  python upload_database.py         # æ­£å¸¸è¿è¡Œ")
            print("  python upload_database.py debug   # è°ƒè¯•ç´¢å¼•å†…å®¹")
            print("  python upload_database.py clear [security|topsec|all]  # æ¸…ç©ºç´¢å¼•")
    else:
        # æ­£å¸¸è¿è¡Œæ¨¡å¼
        main()